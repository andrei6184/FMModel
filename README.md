# FMModel

Проект представляет собой реализацию [Factorization Machine](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) на языке Scala.

## Архитектура приложения:

Утилита имеет два режима работы – обучение и применение. Исходные данные хранятся в текстовых файлах в CSV формате. Общая структура CSV файла следующая:
```
На этапе обучения: CF1, CF2, …, CFN, TARGET
На этапе применения: CF1, CF2, …, CFN
```
где **CFi** – соответствующий категориальный признак (целые числа);
**TARGET** – целевая переменная (вещественная);

Программа состоит из следующих основных компонентов:
1. **Sample** – представляет собой компонент в котором хранится информация об объекте. Данный класс состоит из следующих полей features – признаки объекта в виде набора пар `(номер признака, аккумулятинвая сумма (см.ниже))`. **target** – целевая переменная (в случае работы в процессе использования модели принимает значение NaN).
2. **FileReader** – это класс, который читает данные из файла, и преобразовывает их в набор **Sample**.
Параметрами данного класса являются путь к исходному файлу, а также максимальное количество признаков. В ходе чтения файла данный класс извлекает из строк признаки, хэширует их и создает объект **Sample**. Если включен режим аккумуляции, то в случае если у двух признаков совпадают хэши по модулю максимального количества признаков, то в соответсвующий элемент признаков запишется количество совпадающих хешей.
3. **Model** – представляет собой класс реализующий модель факторизационных машин. Данная модель использует следующую формулу для предсказания значений:

![Formula](https://github.com/andrei6184/FMModel/blob/master/images/formula.png)

Параметрами такой модели является количество признаков *n* и глубина *m*. Под глубиной понимается размер векторов *v*. Внутри модель хранит смещение *b*, вектор весов *w* размера *n*, а также все вектора признаков *v* в виде матрицы размеров n &times; m. Фактически все что умеет модель – это считать предсказания по формуле, приведенной выше и обновлять свои внутрение элементы. Причем для обновления элементов можно использовать представление в разреженном формате. Так если необходимо обновить всего три элемента из тысячи необходимо подать на вход функции обновления всего три пары `(обновляемый индекс элемента; значение на который оно должно уменьшится)`.

4. Также с классом **Model** неразрывно связан абстрактный класс **LossFunction** и две его реализации **MSELossFunction** и **LogLossFunction**. Класс **LossFunction** умеет вычислять функцию потерь, а также соответсвующие градиенты для смещения *b*, вектора *w* и матрицы векторов *v*. **MSELossFunction** и **LogLossFunction** являются его имплементациями для задачи регрессии и классификации соответственно. Параметрами для **LossFunction** являются коэфициенты регуляризации *C1* и *C2*. *C1* – используется для **L2-регуляризации** вектора *w*, а *C2* используется для **L2-регуляризации** матрицы векторов *v*. Тут необходимо отметить, что по умолчанию данные коэффициенты равны 0, т.е. регуляризация отсутствует. Их можно задать через соответствующие аргументы командной строки. Но при использовании регуляризации производительность будет меньше, т.к. необходимо будет модифицировать весь вектор и всю матрицу.

5. **SGD** – это компонент, который обучает модель с помощью метода [стохастического градиентного спуска](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) и возвращает качество модели для переданного набора обучающих объектов. Параметрами для данного компонента является *скорость обучения* и *скорость уменьшения скорости обучения* на каждой итерации.

6. **Parameters** – это компонент который включает в себя все параметры запуска программы. Для установки значений данного компонента используется компонент **CLI**, который парсит переданные программе аргументы и устанавливает соответсвующие значения у **Parameters**.

7. **Main** – это корневой объект, который управляет жизненным циклом программы.

## Процедура сборки проекта.
Для начала вам необходимо скачать проект с [git репозитория](https://github.com/andrei6184/FMModel).
Затем вам необходимо установить [scala версии 2.12.x](https://www.scala-lang.org/download/) и [sbt версии 1.0.x](https://www.scala-sbt.org/download.html). Далее перейти в каталог со скаченным репозиторием и выполнить команду 
```bash
>> sbt assembly
```

В результате произойдет сборка проекта и итоговый **fmmodel.jar** файл будет собран в папке **target\scala-2.12**. Для запуска программы необходимо выполнить следующую команду:
```bash
>> scala fmmodel.jar [args]
```

## Аргументы утилиты
<pre>
usage: FMModel [-a] [-bs <arg>] [-C1 <arg>] [-C2 <arg>] [-d <arg>] -f
       <file> [-h] [-i <arg>] [-l <arg>] [-LD <arg>] [-lg] [-LR <arg>] [-M
       <arg>] [-o <arg>] [-s <arg>] [-t] [-tm]
 -a                             accumulate the number of the features
 -bs,--batchSize <arg>          batch size for training (default value =
                                1)
 -C1 <arg>                      the regularization coefficient of the
                                weights vector (default value = 0.0)
 -C2 <arg>                      the regularization coefficient of the
                                weights matrix (default value = 0.0)
 -d,--depth <arg>               model depth (default value = 2)
 -f,--datafile <file>           get data from this file
 -h,--help                      print help
 -i,--numIter <arg>             number of iterations used for training
                                (default value = 1)
 -l,--load <arg>                load model from file
 -LD,--learningDecrease <arg>   learning decrease factor (default value =
                                0.01)
 -lg                            to use logistic regression
 -LR,--learningRate <arg>       initial learning rate (default value =
                                0.01)
 -M <arg>                       feature size = 2^M (default value = 10
 -o,--output <arg>              output file with prediction results
 -s,--save <arg>                save model to file
 -t,--train                     train model
 -tm                            use the timing
</pre>

## Примеры запуска утилиты.
```bash
>> scala fmmodel.jar -h
```
Выведет параметры запуска данного приложения.

```bash
>> scala fmmodel.jar -f data\ml-100k\train.csv -s models\ml-100k.fm -M 10 -t -bs 100 -i 3 –tm
```
Запустит обучение модели на данных, которые хранятся в *data\ml-100k\train.csv* файле. После обучения модель будет сохранена в *models\ml-100k.fm* файле. Количество признаков у данной модели будет равно 2^10 = 1024, глубина модели по умолчанию равна 2, колечество проходов по данным равно 3. После обучения в консоль будет выведено общее время затраченное на обучение.

```bash
>> scala fmmodel.jar -f data\ml-100k\test.csv -l models\ml-100k.fm -M 10 -tm -o data\ml-100k\ans.fm
```
Данная команда запустит применение модели на данных, которые хранятся в файле *data\ml-100k\test.csv* при этом будет использоваться модель обученая и сохраненная на предыдущем этапе *models\ml-100k.fm*. Результаты применения модели будут сохранены в файле *data\ml-100k\ans.fm*.

## Тестирование утилиты.
Утилита была протестирована на трех наборах данных:
- ml-100k (для задачи регрессии);
- ml-20m (для задачи регрессии);
- avazu (для задачи классификации).

Рассмотрим процесс тестирования для набора *ml-100k* более подробно.
Данный набор состоит из следующих файлов:
**data\ml-100k\train.csv** – содержит данные для обучения:
```
1,1,5
1,2,3
1,3,4
```
где первые две колонки это два категориальных признака (идентификатор пользователя и идентификатор фильма), а последняя колонка – целевой признак для задачи регрессии.

**data\ml-100k\train.vw** – содержит те же данные для обучения Vowpal Wabbit.
```
5 |x 1 1
3 |x 1 2
4 |x 1 3
```

**data\ml-100k\test.csv** – содержит данные для тестирования (категориальные признаки без целевой переменной).
**data\ml-100k\test.vw** – тестовые данные для [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki).
**data\ml-100k\ans.csv** – файл, который содержит правильные ответы для test. Данный файл используется для конечной валидации.

Выполним обучение Vowpal Wabbit модели:
![Train_VW](https://github.com/andrei6184/FMModel/blob/master/images/train-100k.png)

Как видно из приведенного скриншота при построении модели использовалась **–q опция**.

Выполним тестирование построенной модели:
![Test_VW](https://github.com/andrei6184/FMModel/blob/master/images/test-100k.png)

С помощью вспомогательного кода на питоне, который хранится в файле [data-processing.ipynb#Validation](https://github.com/andrei6184/FMModel/blob/master/data/data-processing.ipynb) оценим качество построенной модели:
```
MSE = 1.3770
```

Далее выполним обучение реализации Факторизационных машин:
![Train_FM_01](https://github.com/andrei6184/FMModel/blob/master/images/train-100k-fm.png)
![Train_FM_02](https://github.com/andrei6184/FMModel/blob/master/images/train-100k-fm-02.png)

Как видно в процессе работы программы выводится дополнительная информация о качестве модели после обработки соответствующего батча данных. В первом столбце отображается качество на текущем батче, а во-втором общее качество с начала работы утилиты.

Оценим качество работы данной модели таким же образом, как и для Vowpal Wabbit модели.
```
MSE = 1.2254
```

Как видно, полученные результаты оказались лучше, чем для Vowpal Wabbit модели.
Проведем аналогичное сравнение для наборов **ml-20m**, **avazu**. Получим следующие результаты:

Задача  | VW     | FM
------  | ------ | ---
ml-100k | 1.3770 | 1.2254
ml-20m  | 1.0942 | 1.0874
avazu   | Acc: 0.8405 <br/> ROC AUC: 0.7379 | Acc: 0.8405 <br/> ROC AUC: 0.7357

Как видно, во всех наборах данных наблюдается незначительное улучшение качества для FM по сравнению с VW.
Выполним замеры времени работы FM для исходных наборов при следующих параметрах:
```
-M 10 -t -bs 100 -i 3
```

Задача | Размер обучающей выборки | Время работы (сек)
-- | -- | --
ml-100k | 80000 | ≈ 0.4
ml-20m | 15945812 | ≈ 90
avazu | 32343174 | ≈ 2200

Как видно, что чем больше данных в выборке, тем больше времени необходимо для обучения модели. Причем сложность определяется не только размером выборки, но и типом задачи. Так для задачи регрессии необходимо значительно меньше времени чем для задачи классификации, т.к. классификация использует логистическую функцию потерь, которая значительно сложнее для вычисления, чем квадратичная функция потерь.

## Основные сложности при выполнении проекта

При выполнении проекта передо мной возникли следующие проблемы:
1. Как эффективно считать файл данных больших размеров? Изначально, я пробовал вычитывать весь файл сразу и сохранить его в память, но тут я столкнулся с тем, что чтение файла занимает большое количество времени и, вероятно, пользователю не захочется ждать, пока файл считается. Другой момент заключается в том, что если файл будет ну очень большим, то есть высокий риск, что процесс упадет. Поэтому я отказался от этой идеи и сделал так, чтобы файл читался порциями размеров, задающихся параметром *–bs*. При этом сохранил возможность итеративной обработки.
2. Как эффективно хранить категориальные признаки? Тут я воспользовался классическим решением данной проблемы - [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing).
3. Как правильно реализовать функции потерь? Изначально я пытался отделить данный функционал от модели, но потом я понял, что это невозможно, т.к. в своей работе они должны опираться на конкретные параметры модели. Поэтому я сделал функции потерь в виде внутренних классов, которые могут без проблем обращаться к полям объемлющего класса **Model**.
4. Как быстро обнавлять параметры модели? Очевидно, что если размеры вектора весов и матрицы будут огромны, то каждое обновление всего вектора или матрицы потребует большого времени выполнения. Поэтому я решил делать обновления в разреженном формате. Т.е. в метод обновления передаются, только индексы и значения тех элементов, которые необходимо обновить на данном шаге. Однако при использовании **L2-регуляризации**, как извествно, необходимо обновить весь вектор *w* и матрицу векторов *v* целеком, поэтому я постарался сделать так, чтобы, во-первых, регуляризацию можно было включать либо отключать и, во-вторых, я постарался делать "регуляризационные обновления" как можно реже, а именно в конце обработки целого батча, а не отдельного экземпляра.
